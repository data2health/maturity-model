# Budget
Don't edit this - the RPPR generater populates this section

# Research Design
CTSAs are the core sources of clinical and translational research, but often struggle with helping their organizations understand the strategic importance of improving its informatics capabilities and IT deployment. All institutions are trying to understand what they should invest in to remain innovative and competitive, not only in research but in translating knowledge into practice and exploring the data they have to improve health.

Significant interest in the Maturity Model has already been elicited without a wide call. We want to democratize this kind of survey so that stakeholders at all levels can self-assess and work to improve their current functions and plan for innovation. We also want to expand the coverage of existing maturity models to ensure utility of the models for organizations in meeting data needs for translational science.


# Methodology
Organizations that engage in research, especially those with Clinical and Translational Science groups, may want to self-assess their maturity of key research IT capabilities and learn to improve these capabilities. This project intends to develop an approach to help organizations through that process. 

There are two core components of this project. The first is an extension and translation of model development by Embi, Knosp, Barnett, and Anderson by narrowing the focus to key areas related to collaborative and open science, providing more clarity and context for the possibilities for improvement. It also intends to facilitate the process of improvement through guided vignettes and tools. 

The purpose of this survey is to elicit key indicators related to research and translational IT that may drive the ability for institutions to engage in innovative and collaborative open science. Our definition of research and translational IT are the capabilities that enable data, information, and knowledge to be discovered, processed, and shared. It focuses on three key areas:
- Governance and Leadership
- Data sharing and licensing
- Deployment of capabilities related to data (architecture, content, tools, and sharing)

The survey has three steps:
- Data collection for “artifacts” related to our focus areas and open science/collaboration more broadly. This includes documenting the evidence of tools, policies, governance structures, leadership positions, and resources at a site.
- Open-ended questions related to governance and leadership, and policies influencing data sharing and licensing.
- A guided identification of ‘bright spots’ related to the deployment of key infrastructure capabilities.

The second component of the project involves expanding the coverage of maturity models. This component is based on work within the community, where informatics and translational science experts identified a broad need for definitions of existing best practices and assessments in multiple areas. Based on identified areas, we will organize and prioritize these topics according to needs, match to existing models, and develop simple models that can be rapidly distributed to the community. 

Models created by both methods will be combined in a tool that will allow organizations to choose areas for self-assessment, support assessments, provide feedback and existing comparisons based on distributions from other organizations, and update distributions according to site assessments. 

# Expected Outcomes
- Defined adoption model for research informatics maturity with vignettes 
- Distributions of adoption levels across surveyed sites
- Organization and prioritization of requested maturity topics
- Basic maturity levels for prioritized topics
- Tool supporting self-assessment and assessment collection

# Deliverables

- Defined adoption model for research informatics maturity with vignettes
- Tool supporting self-assessment and assessment collection

# Timeline (monthly)
 Due Date | Milestone    | Status     | 
|:----------|:--------------|------------:|
8/2019 | **Main adoption model:** Vignettes defined for adoption levels | DONE
8/2019 | **Main adoption model:** Distribution of surveyed sites across adoption levels determined | DONE
8/2019 | **Expanded maturity models:** Topics prioritized, grouped and mapped to components of existing models where available | DONE
9/2019 | **Main adoption model:** Adoption model tool as document describing model with descriptions of levels and distributions | DONE
9/2019 | **Expanded maturity models:** Community responses used to complete top and bottom levels of maturity for prioritized topics | DONE
9/2019 | **Software Assessment Tool:** Software requirements and validation | DONE
10/2019 | **Software Assessment Tool:** Assessment tool software development plan | DONE
12/2019 | **Software Assessment Tool:** Version 1 of software developed and ready for pilot testing | DONE

# Potential Pitfalls and Alternative Strategies
By using two approaches for developing maturity models, we mitigate the main pitfalls of models being difficult to define. The main potential pitfall for the main adoption model is that there is too much diversity of approach, which would make creating vignettes and distributions difficult. A pitfall for the rapid development approach is that the models will not be sufficiently detailed to be useful.
